// on -O1+ gcc optimises out loading data to the arrays.
// consider putting testing code in a different source unit.

// making arguments volatile provokes gcc to insert prolog
// and epilog to the function that breaks it.

// int main(void) {
//    double a[8] = {1, 6, 2.6, 8.9, 2.3, 1.8, 2.7, 3.5};
//    double b[8] = {1, 6, 2.6, 3.9, 2.3, 8.8, 2.7, 7.5};
//    double f = dot(a, b, 8);
//    printf("%lf", f);
// }

#include <stddef.h>

__attribute__((naked)) double dot(const double * x, const double * y, size_t n) {
    asm volatile (
    "    vxorpd      %xmm1, %xmm1, %xmm1\n"
    "    vmovapd     %xmm1, %xmm0\n"
    "    shrq        $1, %rdx\n"
    "    je          .emp\n"
    "    cmpq        $16, %rdx\n"
    "    jb          .sblk\n"
    "    movq        %rdx, %rax\n"
    "    xorl        %r8d, %r8d\n"
    "    vxorpd      %ymm5, %ymm5, %ymm5\n"
    "    andq        $-16, %rax\n"
    "    vmovapd     %ymm5, %ymm2\n"
    "    xorl        %ecx, %ecx\n"
    "    vmovapd     %ymm2, %ymm7\n"
    "    vmovapd     %ymm7, %ymm6\n"
    ".fmaloop:\n"
    "    vmovupd     (%rcx,%rdi), %ymm8\n"
    "    addq        $16, %r8\n"
    "    vmovupd     32(%rcx,%rdi), %ymm9\n"
    "    vmovupd     (%rcx,%rsi), %ymm16\n"
    "    vmovupd     32(%rcx,%rsi), %ymm17\n"
    "    vmovupd     64(%rcx,%rsi), %ymm20\n"
    "    vmovupd     96(%rcx,%rsi), %ymm21\n"
    "    vmovupd     64(%rcx,%rdi), %ymm12\n"
    "    vmovupd     96(%rcx,%rdi), %ymm13\n"
    "    vmovupd     192(%rcx,%rdi), %ymm28\n"
    "    vmovupd     224(%rcx,%rdi), %ymm29\n"
    "    vshuff32x4  $0, %ymm9, %ymm8, %ymm10\n"
    "    vshuff32x4  $3, %ymm9, %ymm8, %ymm11\n"
    "    vshuff32x4  $0, %ymm17, %ymm16, %ymm18\n"
    "    vshuff32x4  $3, %ymm17, %ymm16, %ymm19\n"
    "    vshuff32x4  $0, %ymm13, %ymm12, %ymm14\n"
    "    vshuff32x4  $3, %ymm13, %ymm12, %ymm15\n"
    "    vshuff32x4  $0, %ymm21, %ymm20, %ymm22\n"
    "    vshuff32x4  $3, %ymm21, %ymm20, %ymm23\n"
    "    vshuff32x4  $0, %ymm29, %ymm28, %ymm30\n"
    "    vshuff32x4  $3, %ymm29, %ymm28, %ymm31\n"
    "    vmovupd     128(%rcx,%rsi), %ymm8\n"
    "    vmovupd     160(%rcx,%rsi), %ymm9\n"
    "    vmovupd     192(%rcx,%rsi), %ymm12\n"
    "    vmovupd     224(%rcx,%rsi), %ymm13\n"
    "    vunpcklpd   %ymm11, %ymm10, %ymm4\n"
    "    vunpckhpd   %ymm11, %ymm10, %ymm3\n"
    "    vunpcklpd   %ymm19, %ymm18, %ymm24\n"
    "    vunpckhpd   %ymm19, %ymm18, %ymm26\n"
    "    vfmadd213pd %ymm5, %ymm24, %ymm4\n"
    "    vfmadd213pd %ymm7, %ymm26, %ymm3\n"
    "    vmovupd     128(%rcx,%rdi), %ymm5\n"
    "    vmovupd     160(%rcx,%rdi), %ymm7\n"
    "    vshuff32x4  $0, %ymm9, %ymm8, %ymm10\n"
    "    vshuff32x4  $3, %ymm9, %ymm8, %ymm11\n"
    "    vunpcklpd   %ymm15, %ymm14, %ymm1\n"
    "    addq        $256, %rcx\n"
    "    vunpckhpd   %ymm15, %ymm14, %ymm0\n"
    "    vunpcklpd   %ymm23, %ymm22, %ymm25\n"
    "    vunpckhpd   %ymm23, %ymm22, %ymm27\n"
    "    vfmadd213pd %ymm2, %ymm25, %ymm1\n"
    "    vfmadd213pd %ymm6, %ymm27, %ymm0\n"
    "    vshuff32x4  $0, %ymm7, %ymm5, %ymm2\n"
    "    vshuff32x4  $3, %ymm7, %ymm5, %ymm6\n"
    "    vshuff32x4  $0, %ymm13, %ymm12, %ymm14\n"
    "    vshuff32x4  $3, %ymm13, %ymm12, %ymm15\n"
    "    vunpcklpd   %ymm6, %ymm2, %ymm5\n"
    "    vunpckhpd   %ymm6, %ymm2, %ymm7\n"
    "    vunpcklpd   %ymm31, %ymm30, %ymm2\n"
    "    vunpckhpd   %ymm31, %ymm30, %ymm6\n"
    "    vunpcklpd   %ymm11, %ymm10, %ymm16\n"
    "    vunpckhpd   %ymm11, %ymm10, %ymm18\n"
    "    vunpcklpd   %ymm15, %ymm14, %ymm17\n"
    "    vunpckhpd   %ymm15, %ymm14, %ymm19\n"
    "    vfmadd213pd %ymm4, %ymm16, %ymm5\n"
    "    vfmadd213pd %ymm1, %ymm17, %ymm2\n"
    "    vfmadd213pd %ymm3, %ymm18, %ymm7\n"
    "    vfmadd213pd %ymm0, %ymm19, %ymm6\n"
    "    cmpq        %rax, %r8\n"
    "    jb          .fmaloop\n"
    "    vaddpd      %ymm2, %ymm5, %ymm1\n"
    "    vaddpd      %ymm6, %ymm7, %ymm4\n"
    "    vextractf128 $1, %ymm1, %xmm0\n"    // cry about it
    "    vextractf128 $1, %ymm4, %xmm5\n"
    "    vaddpd      %xmm0, %xmm1, %xmm2\n"
    "    vaddpd      %xmm5, %xmm4, %xmm6\n"
    "    vunpckhpd   %xmm2, %xmm2, %xmm3\n"
    "    vunpckhpd   %xmm6, %xmm6, %xmm7\n"
    "    vaddsd      %xmm3, %xmm2, %xmm1\n"
    "    vaddsd      %xmm7, %xmm6, %xmm0\n"
    ".niter:\n"
    "    xorl        %r8d, %r8d\n"
    "    lea         1(%rax), %r9\n"
    "    xorl        %ecx, %ecx\n"
    "    cmpq        %rdx, %r9\n"
    "    ja          .emp\n"
    "    movq        %rax, %r9\n"
    "    subq        %rax, %rdx\n"
    "    shlq        $4, %r9\n"
    "    addq        %r9, %rdi\n"
    "    addq        %r9, %rsi\n"
    ".remainder:\n"
    "    vmovsd      (%rcx,%rsi), %xmm2\n"
    "    incq        %r8\n"
    "    vmovsd      8(%rcx,%rsi), %xmm3\n"
    "    vfmadd231sd (%rcx,%rdi), %xmm2, %xmm1\n"
    "    vfmadd231sd 8(%rcx,%rdi), %xmm3, %xmm0\n"
    "    addq        $16, %rcx\n"
    "    cmpq        %rdx, %r8\n"
    "    jb          .remainder\n"
    ".emp:\n"
    "    vaddsd      %xmm0, %xmm1, %xmm0\n"
    "    vzeroupper\n"
    "    ret\n"
    ".sblk:\n"
    "    xorl        %eax, %eax\n"
    "    jmp         .niter\n"
    );
}
